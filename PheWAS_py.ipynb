{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dbmi.hms.harvard.edu/sites/g/files/mcu781/files/hero-images/HMS_DBMI_Logo.svg\" width= \"550px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIC-SURE API use-case: Phenome-Wide analysis on COPDgene data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIC-SURE python API \n",
    "### What is PIC-SURE? \n",
    "\n",
    "<!--img src=\"./img/PIC-SURE_logo.png\" width= \"360px\"> -->\n",
    "\n",
    "Databases exposed through PIC-SURE API encompass a wide heterogeneity of architecture and data organization underneath. PIC-SURE hide this complexity and expose the different databases in the same format, allowing researchers to focus on the analysis and medical insights, thus easing the process of reproducible sciences.\n",
    "\n",
    "\n",
    "### Why PIC-SURE? \n",
    "Databases exposed through PIC-SURE API encompass a wide heterogeneity of architecture and data organization underneath. PIC-SURE hide this complexity and expose the different data in the same format, allowing for data-scientist and clinical researchers to focus on the analysis and medical insights, thus unburdening the complexity of integrating clinical and genomics data, and easing the process of reproducible sciences.\n",
    "\n",
    "### More about\n",
    "PIC-SURE stands for Patient-centered Information Commons: Standardized Unification of Research Elements. The API is available in two different programming languages, python and R, allowing investigators two query databases in the same way using any of those languages.\n",
    "\n",
    "PIC-SURE is a large project from which the PIC-SURE R/python API is only a brick. Among those other components, PIC-SURE also offers a graphical user interface, allowing scientist to get quick knowledge about variables and data available for a specific data source.\n",
    "\n",
    "The Python/R API is actively developed by the Avillach-Lab at Harvard Medical School.\n",
    "\n",
    "GitHub repo:\n",
    "* https://github.com/hms-dbmi/pic-sure-python-adapter-hpds\n",
    "* https://github.com/hms-dbmi/pic-sure-python-client\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenome-Wide Association Studies (PheWAS)\n",
    "\n",
    "### What is a PheWAS analysis?\n",
    "A PheWAS analysis is basically testing the association of an individual trait (i.e. a genomic variant in most of the cases, but not exclusively) against a wide variety of phenotypes. It is frequently used in the genomics field, sometimes in association with GEWAS analyzes (invert process, that is testing association of a phenotype against multiple genetic variants).\n",
    "\n",
    "References:\n",
    "- [*Denny et al.*, 2010](https://academic.oup.com/bioinformatics/article/26/9/1205/201211)\n",
    "- [*Denny et al.*, 2017](https://www.annualreviews.org/doi/abs/10.1146/annurev-genom-090314-024956)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COPDGene data\n",
    "\n",
    "COPDGene is a case-control study that focus on Chronic Obstructive Pulmonary Disease (COPD), and that comprise linked genomic and clinical data. It's one of the database that is integrated in the BioData Catalyst alongside other projects.\n",
    "Although genomics data are not yet available through PIC-SURE API, COPDGene is well-suited for such a use case because of the fact that it does provide a specific trait (namely presence or absence of a COPD diagnosis) which appears to be relevant to test against every other phenotypical variable availables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -------   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook to be reproducible out of the box in any environment, here is the code to download and install the necessary packages for this analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite\n",
    "- python 3.6 or later (although earlier versions of Python 3 must work too)\n",
    "- pip: python package manager, already available in most system with a python interpreter installed ([pip installation instructions](https://pip.pypa.io/en/stable/installing/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPython magic command\n",
    "\n",
    "Those two lines of code below do load the `autoreload` IPython extension. Although not necessary to execute the rest of the Notebook, it does enable to reload every dependency each time python code is executed, thus enabling to take into account changes in external file imported into this Notebook (e.g. user defined function stored in separate file), without having to manually reload libraries. Turns out very handy when developing interactively. More about [IPython Magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to estimate execution time of the Notebook, see at the end\n",
    "from datetime import datetime\n",
    "then = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of external dependencies\n",
    "\n",
    "Installation of depencides using the pip package manager. \n",
    "<!--`%pip` execute a shell command that install the required libraries for this notebook (FYI, `%pip` install libraries for the current kernel, as opposed to the `!pip` command which install packages for the python interpreter that launched the Notebook, [see](https://stackoverflow.com/questions/38368318/installing-a-pip-package-from-within-a-jupyter-notebook-not-working/50473278#50473278)). -->\n",
    "\n",
    "\n",
    "<!-- *TODO: Test %pip in older version of python interpreter, for instance in auth0 oldest environment version* -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.17.3\n",
    "!pip install matplotlib==3.1.1\n",
    "!pip install pandas==0.25.3\n",
    "!pip install scipy==1.3.1\n",
    "!pip install tqdm==4.38.0\n",
    "!pip install statsmodels==0.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/hms-dbmi/pic-sure-python-adapter-hpds.git\n",
    "!pip install git+https://github.com/hms-dbmi/pic-sure-python-client.git "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the external dependencies, along as user-defined functions written in files located in the `python_lib` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import PicSureHpdsLib\n",
    "import PicSureClient\n",
    "\n",
    "from python_lib.utils import get_multiIndex_variablesTable, get_dic_renaming_vars, match_dummies_to_varNames, joining_variablesTable_onCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NB: This Jupyter Notebook has been written using PIC-SURE API following versions:\\n- PicSureClient: 0.1.0\\n- PicSureHpdsLib: 1.1.0\\n\")\n",
    "print(\"The PIC-SURE API libraries versions you've been downloading are: \\n- PicSureClient: {0}\\n- PicSureHpdsLib: {1}\".format(PicSureClient.__version__, PicSureHpdsLib.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up the options for displaying tables and plots in this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame display options\n",
    "pd.set_option(\"max.rows\", 435)\n",
    "\n",
    "# Matplotlib parameters options\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "fig_size[0] = 14\n",
    "fig_size[1] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to HPDS resources\n",
    "Gaining access to any data source using the API require two steps: \n",
    "* Connecting to a PIC-SURE network\n",
    "* Connecting to an HPDS-hosted resource. Indeed, a PIC-sure network can host several different resources. However, the only resources we will be using hereafter is the COPDGene HPDS-hosted database instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to a PIC-SURE network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to a network, one need two informations: \n",
    "- URL of this network. This notebook example is using COPDGene Dev environment, which URL is https://copdgene-dev.hms.harvard.edu/psamaui/login/?redirection_url=/picsureui/\n",
    "- An authorized individual user token to gain access to the resources of the network through the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to get a token and feed it to the API. First, we will create a blank text file that will be used to store the token right after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "token_path = \"./tokens/copd.txt\"\n",
    "if not os.path.isdir(\"./tokens\"):\n",
    "    os.mkdir(\"./tokens\")\n",
    "if not os.path.isfile(token_path):\n",
    "    open(token_path, \"w+\").close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually get the token, process as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In a web browser, open the COPDGene Dev User Interface: https://copdgene-dev.hms.harvard.edu/psamaui/login/?redirection_url=/picsureui/, and choose one of the available authentication methods to enter it.\n",
    "2. In the user-interface click on USER PROFILE\n",
    "3. Click again on USER PROFILE and then on COPY button\n",
    "4. Back into your Jupyter environment, paste it into the newly created text file (`./tokens/copd.txt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Getting authorization token](img/get_token_screen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token is strictly personal**, be careful not to share it with anyone (thus `./tokens` directory is explicitely excluded in the `.gitignore` file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the token has been copied into the prespecified file, we can read it and feed it to the API as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(token_path) as f:\n",
    "    user_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICSURE_network_URL = \"https://copdgene-dev.hms.harvard.edu/picsure/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the PicSureClient library to create the connection to a PICSure network, as well as the PicSureHpdsLib that handle data extraction from a HPDS-hosted database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = PicSureClient.Client()\n",
    "connection = client.connect(PICSURE_network_URL, user_token, allowSelfSignedSSL=True)\n",
    "#connection.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the COPDGene resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPDGene_resource = \"b6ef7b1a-56f6-11e9-8958-0242c0a83007\"\n",
    "adapter = PicSureHpdsLib.Adapter(connection)\n",
    "resource = adapter.useResource(COPDGene_resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we created an object called `resource`, which is an instance of the `PicSureHpdsLib.Adapter()` class. It is connected to the specific resources we indicated, namely COPDGene hosted database in our case. \n",
    "\n",
    "**This `resource` object is actually the only one we will need to proceed with our analysis thereafter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: As of 11/26/19, user tokens to acces PICSure Network got a very limited validity duration time (they're getting expired in about 20 minutes without any connection activity). In the case you're getting a connection error stating: `ERROR: HTTP response was bad [...] User is not authorized. [Token invalid or expired]`, please get a new token the same way you did it before, and update your `resource` object by re-executing cells above. This is a known issue, and tokens life-expectancy will soon be expanded to a suitable duration to conduct analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting help with the PIC-SURE python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object exposed by the PicSureHpdsLib library got a `help()` method. Calling it will print a helper message about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this output tells us that this `resource` object got 2 methods, and it gives insights about their function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the *variables dictionnary*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once connection to the desired resource has been established, we first need to get a grasp of which variables are available in the database. To this end, we will create a `dictionary` using the `resource` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dictionary` object offers the possibility to retrieve matching records according to a specific term, or to retrieve information about all available variables, using the `find()` method. For instance, looking for variables containing the term `COPD`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = resource.dictionary()\n",
    "lookup = dictionary.find(\"pneumonia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, objects created by the `dictionary.find` exposes the search result using 4 different methods: `.count()`, `.keys()`, `.entries()`, and `.DataFrame()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint({\"Count\": lookup.count(), \n",
    "        \"Keys\": lookup.keys(),\n",
    "        \"Entries\": lookup.entries()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`.DataFrame()` appears as the most useful method for an end-user**. \n",
    "\n",
    "* Various criteria exposed in the dictionary (patientCount, variable type ...) can be subsequently used as selection criteria for variable selection.\n",
    "* Row names of the DataFrame, representing actual variables names, can be used in the query, instead of typing directly the name of the variable in the source code.\n",
    "\n",
    "Variable names, as currently implemented in the API, aren't very practical to use.\n",
    "1. Very long\n",
    "2. Presence of backslashes that prevent from copy-pasting. \n",
    "\n",
    "However, using the dictionary to select variables can definitely help to deal with this pitfall. Hence, one handy way to proceed is to retrieve the whole dictionary in the form of a pandas DataFrame, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_variablesDict = resource.dictionary().find().DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, using the find function without arguments return every entries, as stated in the help below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource.dictionary().help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_variablesDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary currently returned by the API provide various information about the variables, such as:\n",
    "- observationCount: number of entries with non-null value\n",
    "- categorical: type of the variables, True if categorical, False if continuous/numerical\n",
    "- min/max: only provided for non-categorical variables\n",
    "- HpdsDataType: 'phenotypes' or 'genotypes'. Currently COPDGene instance only contains 'phenotypes' variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable dictionary + pandas multiIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though helpful, we can use a simple user-defined function (`get_multiIndex_variablesTable`) to add a little more information and ease dealing with variables names. It takes advantage of pandas MultiIndex functionality [see pandas official documentation on this topic](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html).\n",
    "\n",
    "Although not an official feature of the API, such functionality to quickly scan an select groups of related variables may be integrated at some point. So for now, just printing the 'multiIndexed' variable Dictionary allows to quickly see the tree like organisation of the variables. Moreover, original and simplified variable names are now stored respectively in the \"varName\" and \"simplified_varName\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variablesDict = get_multiIndex_variablesTable(plain_variablesDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variablesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have seen how our entire dictionnary looked, we limit the number of lines to be displayed for the future outputs\n",
    "pd.set_option(\"max.rows\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example to illustrate the ease of use a multiIndex Dictionary in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "medication_history_variables = variablesDict.loc[idx[:,\"Medication history\"],:]\n",
    "medication_history_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although pretty simple, it can be easily combined with other filters to quickly select necessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = resource.query()\n",
    "gender_variables = resource.dictionary().find(\"Sex\").keys()\n",
    "query.select().add(gender_variables)\n",
    "query.getCount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the COPDGene HPDS database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside from the dictionary, the second cornerstone of the API is the `query` object (exposed by a resource object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = resource.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most simple usage of the query object is passing a variable name through the `select` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.select().add(\"\\\\03 Clinical data\\\\SF-36 form\\\\SF-36 Body Pain (BP) score\\\\\")\n",
    "query.getResultsDataFrame()\n",
    "query.getCount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is many different methods provided by the API: `select`, `require`, `anyof`, `filter`, and each one of those methods can be combined with `add` and `delete` to create queries. Moreover, different results can be returned for a single query: `getCount`, `getResults` ... Information about each one of those functions can be found using `help()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, **a simple straightforward workflow is to simply select the desired variables using the Dictionary, enter their names using `query.select().add()`, and then retrieve the data using `query.getResultsDataFrame()` method**.\n",
    "\n",
    "Let's say we are interested in the variables pertaining to the 'Respiratory disease form' category, and that we only want the categorical ones, with at least 4000 non-null values. One simple way to process is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = resource.query()\n",
    "mask_cat = variablesDict[\"categorical\"] == True\n",
    "mask_count = variablesDict[\"observationCount\"] > 4000\n",
    "varnames = variablesDict.loc[idx[:, \"Respiratory disease form\"],:].loc[mask_cat & mask_count, \"varName\"]\n",
    "query.select().add(varnames)\n",
    "query_result = query.getResultsDataFrame()\n",
    "query_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PheWAS analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the relevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, a PheWAS analysis consists of two main steps:\n",
    "- Running univariate tests again every phenotypes variable\n",
    "- Adjusting for multiple testing issue\n",
    "\n",
    "In this example, we will select every phenotype variables available in the Dictionary, except for the variables pertaining to the \"Sub-study ESP LungGO COPDGene\" category (very small population as compared to the COPDGene one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pheno = variablesDict[\"HpdsDataType\"] == \"phenotypes\"\n",
    "mask_substudy = variablesDict.index.get_level_values(0) != \"Sub-study ESP LungGO COPDGene\"\n",
    "mask_vars = mask_pheno & mask_substudy\n",
    "selected_vars = variablesDict.loc[mask_vars, \"varName\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = resource.query()\n",
    "query.select().add(selected_vars)\n",
    "facts = query.getResultsDataFrame(selected_vars).set_index(\"Patient ID\")\n",
    "facts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just check that our query runned the way intended by looking at the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0} rows, {1} columns\".format(*facts.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting variables regarding their types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important step in a PheWAS is to make the distinction between categorical and numerical variables. Again, this distinction is straightforward using the variables dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_categories = variablesDict.loc[mask_vars, \"categorical\"] == True\n",
    "categorical_varnames = variablesDict.loc[mask_vars,:].loc[mask_categories, \"varName\"].tolist()\n",
    "continuous_varnames = variablesDict.loc[mask_vars,:].loc[~mask_categories, \"varName\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the variant trait to study\n",
    "Most of PheWAS use genetic variants as the case-control subpopulations whom association with other phenotypes will be tested. But using the population doesn't have to be dichotomized using a genetic variant (see for example [*Neuraz et al.*, 2013](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003405)). \n",
    "\n",
    "Here we will use the presence or absence of a COPD diagnosis as the variable to dichotomize the population in our subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trait_name = variablesDict.loc[variablesDict[\"simplified_varName\"] == \"00 Affection status\", \"varName\"].values[0]\n",
    "categorical_varnames.remove(trait_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we select adequate subpopulations regarding the chosen variant values (i.e. keeping \"Case\" and \"Control\" individuals, thus discarding \"Other\", \"Control, Exclusionary Disease\", and null values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_trait_name = facts[trait_name].isin([\"Case\", \"Control\"])\n",
    "facts = facts.loc[mask_trait_name,:]\n",
    "print(\"Control: {0} individuals\\nCase: {1} individuals\".format(*facts[trait_name].value_counts().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create dummy variables in order to be able to carry categorical univariate statistical tests, and we store their names in the dictionary alongside corresponding original variables in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facts_dummies = pd.get_dummies(facts, columns=categorical_varnames, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_dummies_varNames = match_dummies_to_varNames(facts.columns,\n",
    "                                                      facts_dummies.columns,\n",
    "                                                      columns=[\"varName\", \"dummies_varName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variablesDict = joining_variablesTable_onCol(variablesDict,\n",
    "                                              matching_dummies_varNames,\n",
    "                                              left_col=\"varName\",\n",
    "                                              right_col=\"varName\",\n",
    "                                              overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variablesDict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this point, each variable present in the facts_dummies dataset will be tested with the selected trait (presence or absence of COPD). \n",
    "\n",
    "Two different association test will be carried out according to variables data types: \n",
    "- Mann-Whitney U test for continuous ones\n",
    "- Fisher exact test for categorical ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative variables: Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = facts_dummies.groupby(trait_name) \n",
    "\n",
    "dic_mannwhitneyu = {}\n",
    "for var in continuous_varnames: \n",
    "    group1, group2 = [group[1].dropna() for group in grouped[var]]\n",
    "    try:\n",
    "        dic_mannwhitneyu[var] = stats.mannwhitneyu(group1, group2).pvalue\n",
    "    except ValueError:\n",
    "        dic_mannwhitneyu[var] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative variables: Fisher Exact test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_categorical_varnames = variablesDict.loc[variablesDict[\"varName\"].isin(categorical_varnames),:]\\\n",
    "[\"dummies_varName\"].values[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisher test for categorical variables\n",
    "from tqdm import tqdm\n",
    "dic_fisher = {}\n",
    "try:\n",
    "    for var in tqdm(dummy_categorical_varnames, position=0, leave=True):\n",
    "        if type(var) != str:\n",
    "            print(\"skipping {0}\".format(var))\n",
    "            continue\n",
    "        elif var not in facts_dummies.columns:\n",
    "            print(\"skipping {0}, not in dataframe columns\".format(var))\n",
    "            continue        \n",
    "        crosstab = pd.crosstab(facts_dummies[var], facts_dummies[trait_name])\n",
    "        if crosstab.shape == (1,2):\n",
    "            dic_fisher[var] = np.NaN\n",
    "        else:\n",
    "            dic_fisher[var] = stats.fisher_exact(crosstab)[1]\n",
    "except AttributeError:\n",
    "    print(\"End of loop tqdm AttributeError catched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate tests distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in dic_mannwhitneyu.values()]).plot.hist(bins=30)\n",
    "plt.suptitle(\"Distribution of individual p-values for Mann-Whintey U test\",\n",
    "             weight=\"bold\",\n",
    "            fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in dic_fisher.values()]).plot.hist(bins=20)\n",
    "plt.suptitle(\"Distribution of individual p-values for Fisher association test\", \n",
    "             size=30,\n",
    "             weight=\"bold\",\n",
    "            fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple hypotheses testing correction: Bonferroni Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle the multiple comparison issue (increase in the probability to \"discover\" false statistical associations, because of the number of tests performed), we will use the Bonferroni correction method. Although many other multiple comparison exist, Bonferroni is the most straightforward to use, because it doesn't require assumptions about variables correlation. Other PheWAS analysis also use False Discovery Rate controlling procedures ([see](https://en.wikipedia.org/wiki/False_discovery_rate))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, Bonferonni allows to calculate a corrected \"statistical significant threshold\" according to the number of test performed. Every p-value below this threshold will be deemed statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging pvalues from different tests\n",
    "dic_pvalues = {**dic_mannwhitneyu, **dic_fisher}\n",
    "df_pvalues = pd.DataFrame.from_dict(dic_pvalues, orient=\"index\", columns=[\"pvalues\"])\\\n",
    ".rename_axis(\"dummies_varName\")\\\n",
    ".reset_index(drop=False)\n",
    "\n",
    "# Adding pvalues results as a new column to variablesDict\n",
    "variablesDict = joining_variablesTable_onCol(variablesDict,\n",
    "                                              df_pvalues,\n",
    "                                              left_col=\"dummies_varName\",\n",
    "                                              right_col=\"dummies_varName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_alpha = 0.05/len(variablesDict[\"pvalues\"])\n",
    "variablesDict[\"p_adj\"] = variablesDict[\"pvalues\"] / len(variablesDict[\"pvalues\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variablesDict[\"pvalues\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variablesDict['log_p'] = -np.log10(variablesDict['pvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variablesDict = variablesDict.sort_index()\n",
    "variablesDict[\"group\"] = variablesDict.reset_index(level=1)[\"level_1\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical synthetic data visualisation of a PheWAS analysis is the Manhattan plot, which plot each one of the tested phenotypes on the X-axis, against -log of pvalues on the Y axis. Usually a horizontal line is drawn to represent the corrected level of significance calculated using an adequate multiple hypothesis correction method (Bonferroni in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = variablesDict[\"pvalues\"].isna()\n",
    "df_results = variablesDict.loc[~mask,:].copy().replace([np.inf, -np.inf], np.nan)\n",
    "df_results[\"ind\"] = np.arange(1, len(df_results)+1)\n",
    "df_grouped = df_results.groupby(('group'))\n",
    "\n",
    "# print(df_grouped.head(10))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "colors = plt.get_cmap('Set1')\n",
    "x_labels = []\n",
    "x_labels_pos = []\n",
    "\n",
    "y_lims = (0,\n",
    "          df_results[\"log_p\"].max(skipna=True) + 20)\n",
    "threshold_top_values = df_results[\"log_p\"].sort_values(ascending=False)[0:6][-1]\n",
    "\n",
    "for num, (name, group) in enumerate(df_grouped):\n",
    "        group.plot(kind='scatter', x='ind', y='log_p',color=colors.colors[num % len(colors.colors)], ax=ax, s=20)\n",
    "        x_labels.append(name)\n",
    "        x_labels_pos.append((group['ind'].iloc[-1] - (group['ind'].iloc[-1] - group['ind'].iloc[0])/2)) # Set label in the middle\n",
    "        \n",
    "        pair_ind = 0 # To shift label which might overlap because to close\n",
    "        for n, row in group.iterrows():\n",
    "            if pair_ind %2 == 0:\n",
    "                shift = 1.1\n",
    "            else:\n",
    "                shift = -1.1\n",
    "            if row[\"log_p\"] > threshold_top_values:\n",
    "                ax.text(row['ind'] + 3, row[\"log_p\"] + 0.05 + shift, row[\"simplified_varName\"], rotation=0, alpha=1, size=8, color=\"black\")\n",
    "                pair_ind += 1\n",
    "                \n",
    "ax.set_xticks(x_labels_pos)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_xlim([0, len(df_results) +1])\n",
    "ax.set_ylim(y_lims)\n",
    "ax.set_ylabel('-log(p-values)', style=\"italic\")\n",
    "ax.set_xlabel('Phenotypes')\n",
    "ax.axhline(y=-np.log10(adjusted_alpha), linestyle=\":\", color=\"black\")\n",
    "plt.xticks(fontsize = 8,rotation=90)\n",
    "plt.yticks(fontsize = 8)\n",
    "plt.title(\"Statistical association between studied allele and phenotypes\", \n",
    "          loc=\"center\",\n",
    "          style=\"oblique\", \n",
    "          fontsize = 20,\n",
    "         y=1)\n",
    "xticks = ax.xaxis.get_major_ticks()\n",
    "xticks[0].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "elapsed = now - then\n",
    "print(elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
